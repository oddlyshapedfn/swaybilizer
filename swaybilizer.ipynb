{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TK0WRTtBoFA"
      },
      "source": [
        "#Swaybilizer\n",
        "Translate and rotate every video frame so that a subject's face and eyes remain level at the center of the frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ultralytics kornia opencv-python\n",
        "from google.colab import files\n",
        "import ultralytics as U\n",
        "import torch\n",
        "import cv2\n",
        "import kornia.geometry as kgeom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo9ZjTC091Di"
      },
      "outputs": [],
      "source": [
        "def translation_matrix(dx, dy, device='cpu'):\n",
        "    return torch.tensor(\n",
        "        [\n",
        "            [1, 0, dx],\n",
        "            [0, 1, dy],\n",
        "            [0, 0, 1]\n",
        "        ],\n",
        "        device=device\n",
        "    ).float()\n",
        "\n",
        "def get_transforms(preds, thresh=0.5, device='cpu'):\n",
        "    pts = preds[0].keypoints.data.clone()\n",
        "    dims = preds[0].orig_shape\n",
        "    imcenter = torch.tensor([dims[1] // 2, dims[0] // 2], device=device)\n",
        "\n",
        "    # Loop over first tensor index which indexes multiple targets\n",
        "    out = []\n",
        "    pts[pts[:, :, 2] < thresh] = float('nan')\n",
        "    for det in pts:\n",
        "        coords = det[..., :-1]\n",
        "        center = -1 * (coords[:3].nanmean(dim=0) - imcenter)\n",
        "\n",
        "        # Calculate the angle from the line segment connecting the eyes\n",
        "        slope = coords[1] - coords[2]\n",
        "        angle = -1* torch.atan2(slope[1], slope[0])\n",
        "\n",
        "        m0 = torch.tensor(\n",
        "            [\n",
        "                [1, 0, center[0]],\n",
        "                [0, 1, center[1]],\n",
        "                [0, 0, 1]\n",
        "            ],\n",
        "            device=device\n",
        "        )\n",
        "        m1 = torch.tensor(\n",
        "            [\n",
        "                [torch.cos(angle), -1 * torch.sin(angle), 0],\n",
        "                [torch.sin(angle),      torch.cos(angle), 0],\n",
        "                [0, 0, 1]\n",
        "            ],\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        t0 = translation_matrix(-1 * dims[1] // 2, -1 * dims[0] // 2, device=device)\n",
        "        t1 = translation_matrix(dims[1] // 2, dims[0] // 2, device=device)\n",
        "\n",
        "        # The transforms we need to perform are...\n",
        "        # 1. Move the stabilized point to the center of the frame\n",
        "        # 2. Move the center of the frame to origin (top left corner)\n",
        "        # 3. Rotate the image around the origin so that the eye points are level\n",
        "        # 4. Undo 2) by moving the image back to the center.\n",
        "        # The standard 3x3 transform matrix rotates about the origin, so to rotate around\n",
        "        # The frame center, we need to move the image there and back (virtually)\n",
        "        transform = t1.matmul(m1.matmul(t0.matmul(m0)))\n",
        "\n",
        "        out.append(transform[None])\n",
        "\n",
        "    stacked = torch.stack(out)\n",
        "    return stacked\n",
        "\n",
        "def shift(img, transforms, device='cpu'):\n",
        "    outs = []\n",
        "    img = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    transforms = transforms.to(device)\n",
        "    for d in transforms:\n",
        "        shifted = kgeom.warp_perspective(img, d, img.shape[-2:], align_corners=True)\n",
        "\n",
        "        outs.append(shifted)\n",
        "    return torch.stack(outs).squeeze(1)\n",
        "\n",
        "def swaybilize(cfg):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = U.YOLO(cfg['MODEL'])\n",
        "\n",
        "    fname = cfg['OUT_NAME']\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = None\n",
        "\n",
        "    last = None\n",
        "    frame_count = 0\n",
        "    reader = cv2.VideoCapture(cfg['IN_NAME'])\n",
        "    while True:\n",
        "        frame_count += 1\n",
        "        ret, frame = reader.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        if frame_count % cfg['DECIMATE'] != 0:\n",
        "            continue\n",
        "\n",
        "        h, w, c = frame.shape\n",
        "        if writer is None:\n",
        "            writer = cv2.VideoWriter(cfg['OUT_NAME'], fourcc, cfg['FPS'], (w, h))\n",
        "\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        preds = model(frame)[0]\n",
        "        if preds.keypoints.data.shape[1] == 0:\n",
        "            continue\n",
        "        d = get_transforms(preds, device=device)\n",
        "\n",
        "        # Average the previous motion vector with the current\n",
        "        # to smooth out the motion\n",
        "        if last is None:\n",
        "            last = d\n",
        "            continue\n",
        "        else:\n",
        "            tmp = d\n",
        "            d = 0.50*last + 0.50*d\n",
        "            last = d\n",
        "        shifted = shift(frame, d, device=device)\n",
        "        frame = shifted[0].permute(1,2,0).to(torch.uint8).cpu().numpy()\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        # frame = cv2.resize(frame, (w, h))\n",
        "\n",
        "        writer.write(frame)\n",
        "\n",
        "    writer.release()\n",
        "    reader.release()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell allows you to upload a video. Generally, webm, mkv, mp4 will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "fname = list(uploaded.keys())[0]\n",
        "with open(fname, 'wb') as userfile:\n",
        "  userfile.write(uploaded[fname])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "  # Name of the input file. Automatically populated from the upload.\n",
        "  # Don't change this.\n",
        "  'IN_NAME': fname,\n",
        "\n",
        "  # Name of the output file. It will have this name when downloaded.\n",
        "  # Don't change this.\n",
        "  'OUT_NAME': 'swaybilized.mp4',\n",
        "\n",
        "  # Label the output video as having this FPS.\n",
        "  # Note this only affects playback speed in your video player. If you want to\n",
        "  # skip some frames to reduce processing time, see \"DECIMATE\"\n",
        "  'FPS': 30,\n",
        "\n",
        "  # Use every DECIMATE frames in the input. 2 uses every other, 4 is every fourth, etc\n",
        "  'DECIMATE': 1,\n",
        "\n",
        "  # Model version to use for pose estimation.\n",
        "  # For faster inference, use \"yolov8n-pose.pt\"\n",
        "  # For better accuracy, try \"yolov8l-pose.pt\"\n",
        "  'MODEL': 'yolov8s-pose.pt',\n",
        "}\n",
        "swaybilize(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files.download(config['OUT_NAME'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
